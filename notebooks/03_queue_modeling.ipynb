{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M/M/c Queue Modeling: Analytical and Simulation Approaches\n",
    "\n",
    "**Author:** fergmlx  \n",
    "**Date:** 2025-12-09  \n",
    "**Course:** Operations Research - Queue Theory\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores M/M/c queueing systems through:\n",
    "1. **Analytical formulas** - Exact mathematical solutions\n",
    "2. **SimPy simulation** - Discrete-event simulation\n",
    "3. **Comparative analysis** - Validation and insights\n",
    "4. **Sensitivity analysis** - Impact of parameter changes\n",
    "\n",
    "### M/M/c Queue Characteristics\n",
    "- **M** (Markovian/Exponential): Interarrival times\n",
    "- **M** (Markovian/Exponential): Service times\n",
    "- **c**: Number of parallel servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import simpy\n",
    "from scipy.special import factorial\n",
    "from scipy.optimize import fsolve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Analytical M/M/c Formulas\n",
    "\n",
    "### Key Performance Metrics\n",
    "\n",
    "For an M/M/c queue with arrival rate Œª and service rate Œº:\n",
    "\n",
    "1. **Traffic intensity**: $\\rho = \\frac{\\lambda}{c\\mu}$\n",
    "2. **Probability of zero customers**: $P_0$\n",
    "3. **Erlang-C formula**: $C(c, \\lambda/\\mu)$ - Probability of waiting\n",
    "4. **Average queue length**: $L_q = C(c, \\lambda/\\mu) \\cdot \\frac{\\rho}{1-\\rho}$\n",
    "5. **Average waiting time in queue**: $W_q = \\frac{L_q}{\\lambda}$\n",
    "6. **Average time in system**: $W = W_q + \\frac{1}{\\mu}$\n",
    "7. **Average customers in system**: $L = \\lambda W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMcQueue:\n",
    "    \"\"\"\n",
    "    Analytical M/M/c Queue Model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arrival_rate : float\n",
    "        Average arrival rate (Œª) - customers per unit time\n",
    "    service_rate : float\n",
    "        Average service rate per server (Œº) - customers per unit time\n",
    "    num_servers : int\n",
    "        Number of parallel servers (c)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, arrival_rate, service_rate, num_servers):\n",
    "        self.lam = arrival_rate\n",
    "        self.mu = service_rate\n",
    "        self.c = num_servers\n",
    "        \n",
    "        # Validate stability condition\n",
    "        if arrival_rate >= num_servers * service_rate:\n",
    "            raise ValueError(f\"System is unstable: Œª={arrival_rate} >= c*Œº={num_servers * service_rate}\")\n",
    "        \n",
    "        self.rho = arrival_rate / (num_servers * service_rate)  # Utilization\n",
    "        self.a = arrival_rate / service_rate  # Offered load\n",
    "        \n",
    "    def calculate_P0(self):\n",
    "        \"\"\"Calculate probability of zero customers in system\"\"\"\n",
    "        sum_term = sum([(self.a ** n) / factorial(n) for n in range(self.c)])\n",
    "        last_term = (self.a ** self.c) / (factorial(self.c) * (1 - self.rho))\n",
    "        P0 = 1 / (sum_term + last_term)\n",
    "        return P0\n",
    "    \n",
    "    def erlang_c(self):\n",
    "        \"\"\"Calculate Erlang-C formula (probability of waiting)\"\"\"\n",
    "        P0 = self.calculate_P0()\n",
    "        numerator = (self.a ** self.c) * P0\n",
    "        denominator = factorial(self.c) * (1 - self.rho)\n",
    "        return numerator / denominator\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"Calculate all performance metrics\"\"\"\n",
    "        P0 = self.calculate_P0()\n",
    "        Pc = self.erlang_c()\n",
    "        \n",
    "        # Queue metrics\n",
    "        Lq = Pc * self.rho / (1 - self.rho)  # Average queue length\n",
    "        Wq = Lq / self.lam  # Average waiting time in queue\n",
    "        \n",
    "        # System metrics\n",
    "        W = Wq + (1 / self.mu)  # Average time in system\n",
    "        L = self.lam * W  # Average customers in system\n",
    "        \n",
    "        # Server utilization\n",
    "        utilization = self.rho\n",
    "        \n",
    "        return {\n",
    "            'P0': P0,\n",
    "            'Erlang_C': Pc,\n",
    "            'Lq': Lq,\n",
    "            'Wq': Wq,\n",
    "            'L': L,\n",
    "            'W': W,\n",
    "            'Utilization': utilization,\n",
    "            'rho': self.rho,\n",
    "            'offered_load': self.a\n",
    "        }\n",
    "    \n",
    "    def print_metrics(self):\n",
    "        \"\"\"Display metrics in a formatted table\"\"\"\n",
    "        metrics = self.calculate_metrics()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"M/M/{self.c} Queue - Analytical Results\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Arrival rate (Œª):          {self.lam:.4f} customers/time\")\n",
    "        print(f\"Service rate (Œº):          {self.mu:.4f} customers/time\")\n",
    "        print(f\"Number of servers (c):     {self.c}\")\n",
    "        print(f\"Offered load (a=Œª/Œº):      {metrics['offered_load']:.4f}\")\n",
    "        print(f\"-\" * 60)\n",
    "        print(f\"Server utilization (œÅ):    {metrics['Utilization']:.4f} ({metrics['Utilization']*100:.2f}%)\")\n",
    "        print(f\"P(0 customers):            {metrics['P0']:.6f}\")\n",
    "        print(f\"Erlang-C (P(wait)):        {metrics['Erlang_C']:.6f}\")\n",
    "        print(f\"-\" * 60)\n",
    "        print(f\"Avg queue length (Lq):     {metrics['Lq']:.4f} customers\")\n",
    "        print(f\"Avg system length (L):     {metrics['L']:.4f} customers\")\n",
    "        print(f\"Avg wait time (Wq):        {metrics['Wq']:.4f} time units\")\n",
    "        print(f\"Avg system time (W):       {metrics['W']:.4f} time units\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Test the analytical model\n",
    "print(\"Analytical M/M/c Queue Model initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Comparing Different Server Configurations\n",
    "\n",
    "Let's analyze how system performance changes with different numbers of servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base parameters\n",
    "lambda_base = 8.0  # 8 customers per hour\n",
    "mu_base = 2.0      # 2 customers per hour per server\n",
    "\n",
    "# Test different server configurations\n",
    "server_configs = [3, 4, 5, 6, 7, 8]\n",
    "results = []\n",
    "\n",
    "for c in server_configs:\n",
    "    try:\n",
    "        queue = MMcQueue(lambda_base, mu_base, c)\n",
    "        metrics = queue.calculate_metrics()\n",
    "        metrics['servers'] = c\n",
    "        results.append(metrics)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipping c={c}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_configs = pd.DataFrame(results)\n",
    "df_configs = df_configs[['servers', 'Utilization', 'Erlang_C', 'Lq', 'Wq', 'L', 'W']]\n",
    "\n",
    "print(\"\\nComparison of Server Configurations\")\n",
    "print(\"=\" * 100)\n",
    "print(df_configs.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Visualizing Performance Metrics\n",
    "\n",
    "Visual comparison of key metrics across different server configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "fig.suptitle('M/M/c Queue Performance Metrics vs Number of Servers', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Average Wait Time in Queue (Wq)\n",
    "axes[0, 0].plot(df_configs['servers'], df_configs['Wq'], 'o-', linewidth=2, markersize=8, color='#e74c3c')\n",
    "axes[0, 0].set_xlabel('Number of Servers (c)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Avg Wait Time (Wq)', fontweight='bold')\n",
    "axes[0, 0].set_title('Average Waiting Time in Queue')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Average Queue Length (Lq)\n",
    "axes[0, 1].plot(df_configs['servers'], df_configs['Lq'], 'o-', linewidth=2, markersize=8, color='#3498db')\n",
    "axes[0, 1].set_xlabel('Number of Servers (c)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Avg Queue Length (Lq)', fontweight='bold')\n",
    "axes[0, 1].set_title('Average Number in Queue')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Server Utilization\n",
    "axes[0, 2].plot(df_configs['servers'], df_configs['Utilization']*100, 'o-', linewidth=2, markersize=8, color='#2ecc71')\n",
    "axes[0, 2].set_xlabel('Number of Servers (c)', fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Utilization (%)', fontweight='bold')\n",
    "axes[0, 2].set_title('Server Utilization Rate')\n",
    "axes[0, 2].axhline(y=80, color='orange', linestyle='--', label='80% Target')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Erlang-C (Probability of Waiting)\n",
    "axes[1, 0].plot(df_configs['servers'], df_configs['Erlang_C']*100, 'o-', linewidth=2, markersize=8, color='#9b59b6')\n",
    "axes[1, 0].set_xlabel('Number of Servers (c)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Probability of Waiting (%)', fontweight='bold')\n",
    "axes[1, 0].set_title('Erlang-C: Probability Customer Must Wait')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Average System Length (L)\n",
    "axes[1, 1].plot(df_configs['servers'], df_configs['L'], 'o-', linewidth=2, markersize=8, color='#f39c12')\n",
    "axes[1, 1].set_xlabel('Number of Servers (c)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Avg System Length (L)', fontweight='bold')\n",
    "axes[1, 1].set_title('Average Customers in System')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Average Time in System (W)\n",
    "axes[1, 2].plot(df_configs['servers'], df_configs['W'], 'o-', linewidth=2, markersize=8, color='#1abc9c')\n",
    "axes[1, 2].set_xlabel('Number of Servers (c)', fontweight='bold')\n",
    "axes[1, 2].set_ylabel('Avg Time in System (W)', fontweight='bold')\n",
    "axes[1, 2].set_title('Average Total Time in System')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- As servers increase, wait time and queue length decrease exponentially\")\n",
    "print(\"- Utilization decreases with more servers (diminishing returns)\")\n",
    "print(\"- There's a trade-off between service level and resource efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. SimPy Discrete-Event Simulation\n",
    "\n",
    "Now let's implement a SimPy simulation to validate our analytical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMcSimulation:\n",
    "    \"\"\"\n",
    "    SimPy-based M/M/c Queue Simulation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    arrival_rate : float\n",
    "        Average arrival rate (Œª)\n",
    "    service_rate : float\n",
    "        Average service rate per server (Œº)\n",
    "    num_servers : int\n",
    "        Number of parallel servers (c)\n",
    "    sim_time : float\n",
    "        Total simulation time\n",
    "    warmup_time : float\n",
    "        Warmup period to exclude from statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, arrival_rate, service_rate, num_servers, sim_time=10000, warmup_time=1000):\n",
    "        self.lam = arrival_rate\n",
    "        self.mu = service_rate\n",
    "        self.c = num_servers\n",
    "        self.sim_time = sim_time\n",
    "        self.warmup_time = warmup_time\n",
    "        \n",
    "        # Statistics collectors\n",
    "        self.wait_times = []\n",
    "        self.system_times = []\n",
    "        self.queue_lengths = []\n",
    "        self.system_lengths = []\n",
    "        self.customers_served = 0\n",
    "        self.total_wait_time = 0\n",
    "        \n",
    "    def customer(self, env, name, servers):\n",
    "        \"\"\"Process representing a customer\"\"\"\n",
    "        arrival_time = env.now\n",
    "        \n",
    "        # Record queue length at arrival (only after warmup)\n",
    "        if arrival_time >= self.warmup_time:\n",
    "            self.queue_lengths.append(len(servers.queue))\n",
    "            self.system_lengths.append(servers.count + len(servers.queue))\n",
    "        \n",
    "        # Request a server\n",
    "        with servers.request() as request:\n",
    "            yield request\n",
    "            \n",
    "            wait_time = env.now - arrival_time\n",
    "            \n",
    "            # Generate service time\n",
    "            service_time = np.random.exponential(1.0 / self.mu)\n",
    "            yield env.timeout(service_time)\n",
    "            \n",
    "            system_time = env.now - arrival_time\n",
    "            \n",
    "            # Record statistics (only after warmup)\n",
    "            if arrival_time >= self.warmup_time:\n",
    "                self.wait_times.append(wait_time)\n",
    "                self.system_times.append(system_time)\n",
    "                self.customers_served += 1\n",
    "                self.total_wait_time += wait_time\n",
    "    \n",
    "    def arrival_process(self, env, servers):\n",
    "        \"\"\"Generate customer arrivals\"\"\"\n",
    "        customer_count = 0\n",
    "        while True:\n",
    "            # Generate interarrival time\n",
    "            interarrival_time = np.random.exponential(1.0 / self.lam)\n",
    "            yield env.timeout(interarrival_time)\n",
    "            \n",
    "            # Create new customer\n",
    "            customer_count += 1\n",
    "            env.process(self.customer(env, f'Customer{customer_count}', servers))\n",
    "    \n",
    "    def run(self, random_seed=42):\n",
    "        \"\"\"Run the simulation\"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        # Create SimPy environment\n",
    "        env = simpy.Environment()\n",
    "        \n",
    "        # Create server resource\n",
    "        servers = simpy.Resource(env, capacity=self.c)\n",
    "        \n",
    "        # Start arrival process\n",
    "        env.process(self.arrival_process(env, servers))\n",
    "        \n",
    "        # Run simulation\n",
    "        env.run(until=self.sim_time)\n",
    "        \n",
    "        return self.calculate_metrics()\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"Calculate performance metrics from simulation data\"\"\"\n",
    "        if self.customers_served == 0:\n",
    "            return None\n",
    "        \n",
    "        Wq_sim = np.mean(self.wait_times)\n",
    "        W_sim = np.mean(self.system_times)\n",
    "        Lq_sim = np.mean(self.queue_lengths)\n",
    "        L_sim = np.mean(self.system_lengths)\n",
    "        \n",
    "        # Calculate utilization (customers served * avg service time / (servers * sim time))\n",
    "        actual_sim_time = self.sim_time - self.warmup_time\n",
    "        utilization_sim = (self.customers_served / self.mu) / (self.c * actual_sim_time)\n",
    "        \n",
    "        # Probability of waiting (proportion with wait time > 0)\n",
    "        prob_wait = sum(1 for w in self.wait_times if w > 0.001) / len(self.wait_times)\n",
    "        \n",
    "        return {\n",
    "            'Wq': Wq_sim,\n",
    "            'W': W_sim,\n",
    "            'Lq': Lq_sim,\n",
    "            'L': L_sim,\n",
    "            'Utilization': utilization_sim,\n",
    "            'Prob_Wait': prob_wait,\n",
    "            'Customers_Served': self.customers_served\n",
    "        }\n",
    "\n",
    "print(\"SimPy M/M/c Simulation Model initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Running Simulation for c=5 Servers\n",
    "\n",
    "Let's run a detailed simulation with 5 servers and compare with analytical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for c=5 servers\n",
    "c_selected = 5\n",
    "lambda_val = 8.0\n",
    "mu_val = 2.0\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Running Simulation: M/M/{c_selected} Queue\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Parameters:\")\n",
    "print(f\"  - Arrival rate (Œª): {lambda_val} customers/hour\")\n",
    "print(f\"  - Service rate (Œº): {mu_val} customers/hour/server\")\n",
    "print(f\"  - Number of servers: {c_selected}\")\n",
    "print(f\"  - Simulation time: 10,000 time units\")\n",
    "print(f\"  - Warmup time: 1,000 time units\")\n",
    "print(f\"\\nRunning simulation... \", end='')\n",
    "\n",
    "# Run analytical model\n",
    "analytical = MMcQueue(lambda_val, mu_val, c_selected)\n",
    "analytical_metrics = analytical.calculate_metrics()\n",
    "\n",
    "# Run simulation\n",
    "simulation = MMcSimulation(lambda_val, mu_val, c_selected, sim_time=10000, warmup_time=1000)\n",
    "sim_metrics = simulation.run(random_seed=42)\n",
    "\n",
    "print(\"Complete!\\n\")\n",
    "\n",
    "# Display analytical results\n",
    "analytical.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Analytical vs Simulation Comparison\n",
    "\n",
    "Detailed comparison to validate simulation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['Avg Wait Time (Wq)', 'Avg System Time (W)', 'Avg Queue Length (Lq)', \n",
    "               'Avg System Length (L)', 'Utilization (œÅ)', 'Prob. of Waiting'],\n",
    "    'Analytical': [\n",
    "        analytical_metrics['Wq'],\n",
    "        analytical_metrics['W'],\n",
    "        analytical_metrics['Lq'],\n",
    "        analytical_metrics['L'],\n",
    "        analytical_metrics['Utilization'],\n",
    "        analytical_metrics['Erlang_C']\n",
    "    ],\n",
    "    'Simulation': [\n",
    "        sim_metrics['Wq'],\n",
    "        sim_metrics['W'],\n",
    "        sim_metrics['Lq'],\n",
    "        sim_metrics['L'],\n",
    "        sim_metrics['Utilization'],\n",
    "        sim_metrics['Prob_Wait']\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison['Difference (%)'] = abs(df_comparison['Analytical'] - df_comparison['Simulation']) / df_comparison['Analytical'] * 100\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"Analytical vs Simulation Comparison (c={c_selected})\")\n",
    "print(f\"{'='*90}\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"\\nSimulation Details:\")\n",
    "print(f\"  - Customers served: {sim_metrics['Customers_Served']:,}\")\n",
    "print(f\"  - Mean absolute error: {df_comparison['Difference (%)'].mean():.2f}%\")\n",
    "print(f\"\\nConclusion: {'Excellent' if df_comparison['Difference (%)'].mean() < 5 else 'Good' if df_comparison['Difference (%)'].mean() < 10 else 'Fair'} agreement between analytical and simulation results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot comparison\n",
    "metrics_to_plot = ['Avg Wait Time (Wq)', 'Avg Queue Length (Lq)', 'Utilization (œÅ)', 'Prob. of Waiting']\n",
    "indices = [0, 2, 4, 5]\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "analytical_vals = [df_comparison.iloc[i]['Analytical'] for i in indices]\n",
    "simulation_vals = [df_comparison.iloc[i]['Simulation'] for i in indices]\n",
    "\n",
    "axes[0].bar(x - width/2, analytical_vals, width, label='Analytical', color='#3498db', alpha=0.8)\n",
    "axes[0].bar(x + width/2, simulation_vals, width, label='Simulation', color='#e74c3c', alpha=0.8)\n",
    "axes[0].set_xlabel('Metric', fontweight='bold')\n",
    "axes[0].set_ylabel('Value', fontweight='bold')\n",
    "axes[0].set_title(f'Analytical vs Simulation: M/M/{c_selected} Queue', fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([m.split('(')[0].strip() for m in metrics_to_plot], rotation=15, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Error percentage plot\n",
    "colors = ['green' if x < 5 else 'orange' if x < 10 else 'red' for x in df_comparison['Difference (%)']]\n",
    "axes[1].barh(df_comparison['Metric'], df_comparison['Difference (%)'], color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('Difference (%)', fontweight='bold')\n",
    "axes[1].set_title('Percentage Difference: |Analytical - Simulation|', fontweight='bold')\n",
    "axes[1].axvline(x=5, color='orange', linestyle='--', linewidth=1, label='5% threshold')\n",
    "axes[1].axvline(x=10, color='red', linestyle='--', linewidth=1, label='10% threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Distribution Analysis from Simulation\n",
    "\n",
    "Examine the distributions of wait times and queue lengths from simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Wait time distribution\n",
    "axes[0, 0].hist(simulation.wait_times, bins=50, density=True, alpha=0.7, color='#3498db', edgecolor='black')\n",
    "axes[0, 0].axvline(analytical_metrics['Wq'], color='red', linestyle='--', linewidth=2, label=f\"Analytical: {analytical_metrics['Wq']:.3f}\")\n",
    "axes[0, 0].axvline(sim_metrics['Wq'], color='green', linestyle='--', linewidth=2, label=f\"Simulation: {sim_metrics['Wq']:.3f}\")\n",
    "axes[0, 0].set_xlabel('Wait Time (Wq)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Density', fontweight='bold')\n",
    "axes[0, 0].set_title('Distribution of Wait Times in Queue', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# System time distribution\n",
    "axes[0, 1].hist(simulation.system_times, bins=50, density=True, alpha=0.7, color='#e74c3c', edgecolor='black')\n",
    "axes[0, 1].axvline(analytical_metrics['W'], color='red', linestyle='--', linewidth=2, label=f\"Analytical: {analytical_metrics['W']:.3f}\")\n",
    "axes[0, 1].axvline(sim_metrics['W'], color='green', linestyle='--', linewidth=2, label=f\"Simulation: {sim_metrics['W']:.3f}\")\n",
    "axes[0, 1].set_xlabel('System Time (W)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Density', fontweight='bold')\n",
    "axes[0, 1].set_title('Distribution of Total Time in System', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Queue length distribution\n",
    "queue_counts = pd.Series(simulation.queue_lengths).value_counts().sort_index()\n",
    "axes[1, 0].bar(queue_counts.index, queue_counts.values / len(simulation.queue_lengths), \n",
    "               alpha=0.7, color='#2ecc71', edgecolor='black')\n",
    "axes[1, 0].axvline(analytical_metrics['Lq'], color='red', linestyle='--', linewidth=2, label=f\"Analytical: {analytical_metrics['Lq']:.3f}\")\n",
    "axes[1, 0].axvline(sim_metrics['Lq'], color='blue', linestyle='--', linewidth=2, label=f\"Simulation: {sim_metrics['Lq']:.3f}\")\n",
    "axes[1, 0].set_xlabel('Queue Length (Lq)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Probability', fontweight='bold')\n",
    "axes[1, 0].set_title('Distribution of Queue Length', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# System length distribution\n",
    "system_counts = pd.Series(simulation.system_lengths).value_counts().sort_index()\n",
    "axes[1, 1].bar(system_counts.index, system_counts.values / len(simulation.system_lengths), \n",
    "               alpha=0.7, color='#f39c12', edgecolor='black')\n",
    "axes[1, 1].axvline(analytical_metrics['L'], color='red', linestyle='--', linewidth=2, label=f\"Analytical: {analytical_metrics['L']:.3f}\")\n",
    "axes[1, 1].axvline(sim_metrics['L'], color='blue', linestyle='--', linewidth=2, label=f\"Simulation: {sim_metrics['L']:.3f}\")\n",
    "axes[1, 1].set_xlabel('System Length (L)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Probability', fontweight='bold')\n",
    "axes[1, 1].set_title('Distribution of Customers in System', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistribution Statistics:\")\n",
    "print(f\"  Wait Time - Mean: {np.mean(simulation.wait_times):.4f}, Std: {np.std(simulation.wait_times):.4f}\")\n",
    "print(f\"  System Time - Mean: {np.mean(simulation.system_times):.4f}, Std: {np.std(simulation.system_times):.4f}\")\n",
    "print(f\"  Queue Length - Mean: {np.mean(simulation.queue_lengths):.4f}, Std: {np.std(simulation.queue_lengths):.4f}\")\n",
    "print(f\"  System Length - Mean: {np.mean(simulation.system_lengths):.4f}, Std: {np.std(simulation.system_lengths):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Sensitivity Analysis on Arrival Rates\n",
    "\n",
    "Examine how system performance changes as arrival rate increases (keeping service rate and servers constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis parameters\n",
    "c_fixed = 5\n",
    "mu_fixed = 2.0\n",
    "lambda_range = np.linspace(1.0, 9.5, 20)  # Max is 10 (c*Œº) for stability\n",
    "\n",
    "sensitivity_results = []\n",
    "\n",
    "print(\"\\nPerforming sensitivity analysis on arrival rate...\")\n",
    "print(f\"Fixed parameters: c={c_fixed}, Œº={mu_fixed}\")\n",
    "print(f\"Testing Œª from {lambda_range[0]:.1f} to {lambda_range[-1]:.1f}\\n\")\n",
    "\n",
    "for lam in lambda_range:\n",
    "    try:\n",
    "        queue = MMcQueue(lam, mu_fixed, c_fixed)\n",
    "        metrics = queue.calculate_metrics()\n",
    "        metrics['lambda'] = lam\n",
    "        metrics['rho_percent'] = metrics['Utilization'] * 100\n",
    "        sensitivity_results.append(metrics)\n",
    "    except ValueError:\n",
    "        print(f\"Skipping Œª={lam:.2f} (unstable system)\")\n",
    "        break\n",
    "\n",
    "df_sensitivity = pd.DataFrame(sensitivity_results)\n",
    "\n",
    "print(f\"Completed {len(sensitivity_results)} analysis points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(f'Sensitivity Analysis: Impact of Arrival Rate (Œª) on M/M/{c_fixed} Queue', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Wait Time vs Arrival Rate\n",
    "axes[0, 0].plot(df_sensitivity['lambda'], df_sensitivity['Wq'], 'o-', linewidth=2, markersize=6, color='#e74c3c')\n",
    "axes[0, 0].set_xlabel('Arrival Rate (Œª)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Avg Wait Time (Wq)', fontweight='bold')\n",
    "axes[0, 0].set_title('Wait Time Increases Rapidly Near Capacity')\n",
    "axes[0, 0].axvline(x=c_fixed*mu_fixed, color='red', linestyle='--', alpha=0.5, label=f'Capacity (c¬∑Œº={c_fixed*mu_fixed})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Queue Length vs Arrival Rate\n",
    "axes[0, 1].plot(df_sensitivity['lambda'], df_sensitivity['Lq'], 'o-', linewidth=2, markersize=6, color='#3498db')\n",
    "axes[0, 1].set_xlabel('Arrival Rate (Œª)', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Avg Queue Length (Lq)', fontweight='bold')\n",
    "axes[0, 1].set_title('Queue Length Grows Exponentially')\n",
    "axes[0, 1].axvline(x=c_fixed*mu_fixed, color='red', linestyle='--', alpha=0.5, label=f'Capacity (c¬∑Œº={c_fixed*mu_fixed})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Utilization vs Arrival Rate\n",
    "axes[1, 0].plot(df_sensitivity['lambda'], df_sensitivity['rho_percent'], 'o-', linewidth=2, markersize=6, color='#2ecc71')\n",
    "axes[1, 0].set_xlabel('Arrival Rate (Œª)', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Utilization (%)', fontweight='bold')\n",
    "axes[1, 0].set_title('Server Utilization Increases Linearly')\n",
    "axes[1, 0].axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='80% Target')\n",
    "axes[1, 0].axhline(y=90, color='red', linestyle='--', alpha=0.7, label='90% High Load')\n",
    "axes[1, 0].axvline(x=c_fixed*mu_fixed, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Erlang-C vs Arrival Rate\n",
    "axes[1, 1].plot(df_sensitivity['lambda'], df_sensitivity['Erlang_C']*100, 'o-', linewidth=2, markersize=6, color='#9b59b6')\n",
    "axes[1, 1].set_xlabel('Arrival Rate (Œª)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Probability of Waiting (%)', fontweight='bold')\n",
    "axes[1, 1].set_title('Probability Customer Must Wait')\n",
    "axes[1, 1].axvline(x=c_fixed*mu_fixed, color='red', linestyle='--', alpha=0.5, label=f'Capacity (c¬∑Œº={c_fixed*mu_fixed})')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table for key arrival rates\n",
    "key_lambdas = [4.0, 6.0, 8.0, 9.0, 9.5]\n",
    "summary_data = []\n",
    "\n",
    "for lam in key_lambdas:\n",
    "    try:\n",
    "        queue = MMcQueue(lam, mu_fixed, c_fixed)\n",
    "        metrics = queue.calculate_metrics()\n",
    "        summary_data.append({\n",
    "            'Œª': lam,\n",
    "            'œÅ (%)': f\"{metrics['Utilization']*100:.1f}\",\n",
    "            'Wq': f\"{metrics['Wq']:.4f}\",\n",
    "            'Lq': f\"{metrics['Lq']:.4f}\",\n",
    "            'P(wait)': f\"{metrics['Erlang_C']*100:.2f}%\"\n",
    "        })\n",
    "    except ValueError:\n",
    "        summary_data.append({\n",
    "            'Œª': lam,\n",
    "            'œÅ (%)': 'UNSTABLE',\n",
    "            'Wq': '-',\n",
    "            'Lq': '-',\n",
    "            'P(wait)': '-'\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Sensitivity Analysis Summary: M/M/{c_fixed} with Œº={mu_fixed}\")\n",
    "print(\"=\"*70)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Key Insights from Sensitivity Analysis:\")\n",
    "print(\"\\n1. **Non-linear Response**: As arrival rate approaches capacity (c¬∑Œº=10),\")\n",
    "print(\"   wait times and queue lengths increase exponentially.\")\n",
    "print(\"\\n2. **Critical Threshold**: Around 80-85% utilization, system performance\")\n",
    "print(\"   begins to degrade significantly.\")\n",
    "print(\"\\n3. **Stability Boundary**: System becomes unstable when Œª ‚â• c¬∑Œº.\")\n",
    "print(\"\\n4. **Practical Recommendation**: Operate below 80% utilization for\")\n",
    "print(\"   acceptable service levels and system stability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Multiple Replications Analysis\n",
    "\n",
    "Run multiple simulation replications to assess variability and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple replications\n",
    "num_replications = 30\n",
    "c_test = 5\n",
    "lambda_test = 8.0\n",
    "mu_test = 2.0\n",
    "\n",
    "print(f\"\\nRunning {num_replications} simulation replications...\")\n",
    "print(f\"Configuration: M/M/{c_test}, Œª={lambda_test}, Œº={mu_test}\\n\")\n",
    "\n",
    "replication_results = []\n",
    "\n",
    "for i in range(num_replications):\n",
    "    sim = MMcSimulation(lambda_test, mu_test, c_test, sim_time=10000, warmup_time=1000)\n",
    "    metrics = sim.run(random_seed=i)\n",
    "    metrics['replication'] = i + 1\n",
    "    replication_results.append(metrics)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Completed {i + 1}/{num_replications} replications\")\n",
    "\n",
    "df_replications = pd.DataFrame(replication_results)\n",
    "\n",
    "print(\"\\nReplication analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of replications\n",
    "stats_summary = df_replications[['Wq', 'Lq', 'W', 'L', 'Utilization']].describe()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Statistical Summary of Simulation Replications\")\n",
    "print(\"=\"*80)\n",
    "print(stats_summary)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "\n",
    "print(f\"\\n95% Confidence Intervals (n={num_replications} replications):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for metric in ['Wq', 'Lq', 'W', 'L', 'Utilization']:\n",
    "    data = df_replications[metric]\n",
    "    mean = data.mean()\n",
    "    sem = scipy_stats.sem(data)\n",
    "    ci = scipy_stats.t.interval(confidence_level, len(data)-1, loc=mean, scale=sem)\n",
    "    \n",
    "    print(f\"{metric:15s}: {mean:.6f} ¬± {(ci[1]-mean):.6f}  [{ci[0]:.6f}, {ci[1]:.6f}]\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Compare with analytical\n",
    "analytical_comp = MMcQueue(lambda_test, mu_test, c_test)\n",
    "analytical_metrics_comp = analytical_comp.calculate_metrics()\n",
    "\n",
    "print(\"\\nComparison with Analytical Results:\")\n",
    "print(\"-\" * 80)\n",
    "for metric in ['Wq', 'Lq', 'W', 'L', 'Utilization']:\n",
    "    sim_mean = df_replications[metric].mean()\n",
    "    analytical_val = analytical_metrics_comp[metric]\n",
    "    diff_pct = abs(sim_mean - analytical_val) / analytical_val * 100\n",
    "    \n",
    "    print(f\"{metric:15s}: Analytical={analytical_val:.6f}, Simulation={sim_mean:.6f}, Diff={diff_pct:.2f}%\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize replication variability\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(f'Variability Across {num_replications} Simulation Replications', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Box plots for key metrics\n",
    "metrics_to_plot = [('Wq', 'Avg Wait Time'), ('Lq', 'Avg Queue Length'), \n",
    "                   ('W', 'Avg System Time'), ('Utilization', 'Utilization')]\n",
    "\n",
    "for idx, (metric, label) in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Box plot\n",
    "    bp = ax.boxplot([df_replications[metric]], labels=[label], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('#3498db')\n",
    "    bp['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    # Add analytical line\n",
    "    analytical_val = analytical_metrics_comp[metric]\n",
    "    ax.axhline(y=analytical_val, color='red', linestyle='--', linewidth=2, label='Analytical')\n",
    "    \n",
    "    # Add mean line\n",
    "    sim_mean = df_replications[metric].mean()\n",
    "    ax.axhline(y=sim_mean, color='green', linestyle='--', linewidth=2, label='Sim Mean')\n",
    "    \n",
    "    ax.set_ylabel(label, fontweight='bold')\n",
    "    ax.set_title(f'{label} Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Replication analysis shows consistent results across multiple runs.\")\n",
    "print(\"   The narrow confidence intervals indicate high precision in our estimates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Conclusions and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Model Validation**: Simulation results closely match analytical predictions, validating both approaches.\n",
    "\n",
    "2. **Server Configuration**: \n",
    "   - More servers reduce wait times but increase costs\n",
    "   - Optimal configuration balances service level and resource utilization\n",
    "   - Target utilization: 70-80% for good service with efficiency\n",
    "\n",
    "3. **Sensitivity to Load**:\n",
    "   - System performance degrades rapidly as utilization exceeds 80%\n",
    "   - Operating near capacity leads to exponentially increasing wait times\n",
    "   - Small increases in arrival rate can cause dramatic performance drops\n",
    "\n",
    "4. **Simulation Insights**:\n",
    "   - Multiple replications provide confidence in estimates\n",
    "   - Discrete-event simulation captures operational variability\n",
    "   - Useful for scenarios where analytical solutions are complex\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Capacity Planning**: Maintain 20-30% capacity buffer above expected demand\n",
    "2. **Performance Monitoring**: Track utilization and wait times continuously\n",
    "3. **Dynamic Staffing**: Adjust server count based on demand patterns\n",
    "4. **Service Level Targets**: Set realistic targets based on cost-service trade-offs\n",
    "\n",
    "### Future Work\n",
    "\n",
    "- Explore non-exponential service time distributions (M/G/c queues)\n",
    "- Implement priority queuing systems\n",
    "- Analyze time-varying arrival rates\n",
    "- Optimize staffing schedules with cost constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìö Summary of Analyses Performed:\")\n",
    "print(\"  ‚úì Analytical M/M/c queue formulas implemented\")\n",
    "print(\"  ‚úì Multiple server configurations compared\")\n",
    "print(\"  ‚úì Performance metrics visualized\")\n",
    "print(\"  ‚úì SimPy discrete-event simulation developed\")\n",
    "print(\"  ‚úì Analytical vs simulation validation completed\")\n",
    "print(\"  ‚úì Sensitivity analysis on arrival rates performed\")\n",
    "print(\"  ‚úì Multiple replications for statistical confidence\")\n",
    "print(\"\\nüéØ Key Takeaway: M/M/c queues can be analyzed both analytically and\")\n",
    "print(\"   through simulation, each providing valuable insights for system design.\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
